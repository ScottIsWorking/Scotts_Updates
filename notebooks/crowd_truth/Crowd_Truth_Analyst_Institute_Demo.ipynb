{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyst Institute - Message Testing \"Afraid\" Ad - In your own Words #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from crowd_truth_amt import CrowdTruth\n",
    "from itertools import combinations\n",
    "\n",
    "import qgrid\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the first turk jobs for the `message_testing_afraid_ad_saying_in_your_words` survey items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='indexing responses', max=987.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ai_afraid_ad_own_words_1_path = 'analyst_institute_afraid_ad_message_testing_saying_in_your_own_words_1.csv'\n",
    "ct = CrowdTruth(df=ai_afraid_ad_own_words_1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Asymmetric Worker-Worker Agreement*\n",
    "For workers, $a$ and $b$, it is the number of times that $a$ and $b$ provided the same label for the same item divided by the total number of items that $a$ provided.\n",
    "\n",
    "***What it's good for***: This measure allows us to determine whether a pair of annotators have given the same response on a suspiciously high number of items. This can happen when turk workers have multiple accounts open for the same HITs and use this to double the amount of money they make. It's important to note that this is likely not much of an issue for us since we filter Turk workers for quality before allowing them into the task, so this measure will likely not get used much.\n",
    "\n",
    "***What to look for***: The more items that two annotators have in common, the less likely they are to have nearly perfect alignment. Look for workers who have labeled many things together AND still have a high agreement score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_ids = ct.get_all_worker_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b1053160eb4fb88cbb4756f31bbbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=78.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "worker_combos = list(combinations(worker_ids, 2))\n",
    "\n",
    "worker_agreements = []\n",
    "\n",
    "for wid1, wid2 in tqdm(worker_combos):\n",
    "    w_df1 = ct.get_worker_df(wid1)\n",
    "    w_df2 = ct.get_worker_df(wid2)\n",
    "    items_in_common = ct.items_in_common(w_df1, w_df2)\n",
    "    result1 = {'wid1':wid1,\n",
    "              'wid2':wid2,\n",
    "              'agreement':ct.asym_worker_agreement(w_df1, w_df2),\n",
    "              'n_common':items_in_common}\n",
    "    result2 = {'wid1':wid2,\n",
    "              'wid2':wid1,\n",
    "              'agreement':ct.asym_worker_agreement(w_df2, w_df1),\n",
    "              'n_common':items_in_common}\n",
    "    worker_agreements.append(result1)\n",
    "    worker_agreements.append(result2)\n",
    "    \n",
    "worker_agr_df = pd.DataFrame(worker_agreements)\n",
    "qgrid.show_grid(worker_agr_df.sort_values(by=['agreement', 'n_common'], ascending=[False, False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker-Item Similarity\n",
    "*worker-item similarity* is how close a worker's responses are to the aggregate of everyone else's responses\n",
    "\n",
    "$$\\text{worker-item similarity}(w) = \\text{cosine similarity}(w_i, V_i)$$\n",
    "\n",
    "$$w = \\text{the worker}$$\n",
    "\n",
    "$$w_i = \\text{The label vector that worker $w$ provided to item, $i$}$$\n",
    "\n",
    "$$V_i = \\text{The sum of all label vectors provided by all workers for item, $i$}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for wid in tqdm(worker_ids, desc='Workers'):\n",
    "    w_df = ct.get_worker_df(wid)\n",
    "    \n",
    "    worker_item_sims = ct.get_worker_item_sims(w_df)\n",
    "    \n",
    "    result = {'wid':wid,\n",
    "              'avg_sim': worker_item_sims.mean(),\n",
    "              'stdev_sim': worker_item_sims.std(),\n",
    "              'num_annotations': len(worker_item_sims)}\n",
    "    results.append(result)\n",
    "    \n",
    "results_df = pd.DataFrame(results).set_index('wid')\n",
    "qgrid.show_grid(results_df.sort_values(by=['avg_sim', 'stdev_sim', 'num_annotations'], \\\n",
    "                                       ascending=[True, True,False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing specific workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a distribution plot of the labels a worker provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df = ct.get_worker_df('AEF601SQFOSBL')\n",
    "w_df = w_df[w_df.sum(axis=1) > 0]\n",
    "item_ids = list(w_df.index)\n",
    "w_item_sims = pd.DataFrame([ct.worker_item_sim(w_df, item_id) for item_id in item_ids], index=item_ids, columns=['worker-item sims'])\n",
    "\n",
    "w_df = pd.merge(w_df, w_item_sims, left_index=True, right_index=True)\n",
    "w_df.sum(axis=0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing a Worker\n",
    "If a worker has systematically provided low quality answers, we can remove them with the following steps:\n",
    "1. update `CrowdTruth.df` by removing the worker\n",
    "2. recompute `clarity_df` with `CrowdTruth.compute_clarity_df(df)`\n",
    "\n",
    "In this example, we'll remove worker #3 because they have a very low *worker-item score* AND they labeled a lot of items so this low score has a big influence\n",
    "on the quality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.df = ct.df[ct.df['WorkerId'] != 3]\n",
    "ct.clarity_df = ct.compute_clarity_df(ct.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we review the workers again, we see that the numbers have changed to reflect the absence of worker #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "worker_ids = ct.get_all_worker_ids()\n",
    "\n",
    "for wid in tqdm(worker_ids, desc='Workers'):\n",
    "    w_df = ct.get_worker_df(wid)\n",
    "    \n",
    "    worker_item_sims = ct.get_worker_item_sims(w_df)\n",
    "    \n",
    "    result = {'wid':wid,\n",
    "              'avg_sim': worker_item_sims.mean(),\n",
    "              'stdev_sim': worker_item_sims.std(),\n",
    "              'num_annotations': len(worker_item_sims)}\n",
    "    results.append(result)\n",
    "    \n",
    "results_df = pd.DataFrame(results).set_index('wid')\n",
    "qgrid.show_grid(results_df.sort_values(by=['avg_sim', 'stdev_sim', 'num_annotations'], \\\n",
    "                                       ascending=[True, True,False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: Sometimes when a `0` or `NaN` value is given for `avg_sim` it is the result of the worker's contributions having no\n",
    "overlap with other workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Themes / Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Corroboration Scores\n",
    "The Relation Corroboration score is the number of times that a relation was applied *AND* corroborated by at least $x$ other annotators divided by the overall\n",
    "number of times that the relation was applied.\n",
    "\n",
    "$$\\frac{\\text{number of times the label was corroborated}}{\\text{total number of times the label was applied}}$$\n",
    "\n",
    "This metric allows us to better understand when a theme is clear enough that when it is applied, it is likely that another $x$ annotators will agree that this was a \n",
    "good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.get_theme_corroboration_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theme Association Rules\n",
    "\n",
    "Theme Association Rules are items that appear regularly with each other. It leverages the *apriori* algorithm for association rule mining. We can use this to better understand what themes are being applied regularly with other themes to understand if annotators are effectively distinguishing between themes that should be \n",
    "clearly distinguishable. This can also be used to identify scenarios in which a theme should be a subset of another theme meaning that every time the subtheme is applied,\n",
    "it is implied that the supertheme applies as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.get_theme_association_rules().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Clarity Scores\n",
    "The relation clarity score tells us the most a relation/theme ever uniquely described an item. In our current workflow, we often assume that an item is likely to have more than 1 of the nearly 20 or so themes that are often being applied, so a metric that only tells us how well a relation was every uniquely applied is not helpful as a universal metric for assessing themes. However, it does help us to review relation that we believe *should* be uniquely describing an item. This especially important for non-theme relations like ***NoMeaning***, because we would expect that if we are certain that there is no meaning to a post, then we should not also be choosing a relation that *has meaning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.relation_clarity_scores().sort_values(by='rel_clarity (mean)', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Item Corroboration Score*'\n",
    "the number of times a label was corroborated for an item divided by the total number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(ct.get_item_corroboration_scores().to_frame(name='corroboration'), ct.responses.to_frame(name='response'), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Item-Relation Score*\n",
    "This is the proportion of votes for a relation to apply to a theme. Similar to the issues with the Theme clarity score, Item-Relation Scores mostly help us to understand\n",
    "when a relation *uniquely* describes an item, not when a relation describes an item well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_rel_df = ct.get_item_relation_scores()\n",
    "item_rel_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Item-Clarity*\n",
    "The maximum item-relation score for an item.\n",
    "\n",
    "***Caveat***: Item clarity assumes that what we want is for a relation/theme to singularly represent an item. Ideally, we would use a metric that provides an equally high score for an item that has 1 clear label as it does for an item that has 2 clear items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_rel_df.loc['301KG0KX9DZJQ5F4XJZ78S503L6H2A']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "eda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
